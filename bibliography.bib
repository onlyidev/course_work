@online{andersonLearningEvadeStatic2018,
  title = {Learning to {{Evade Static PE Machine Learning Malware Models}} via {{Reinforcement Learning}}},
  author = {Anderson, Hyrum S. and Kharkar, Anant and Filar, Bobby and Evans, David and Roth, Phil},
  date = {2018-01-30},
  eprint = {1801.08917},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1801.08917},
  url = {http://arxiv.org/abs/1801.08917},
  urldate = {2024-09-30},
  abstract = {Machine learning is a popular approach to signatureless malware detection because it can generalize to never-before-seen malware families and polymorphic strains. This has resulted in its practical use for either primary detection engines or for supplementary heuristic detection by anti-malware vendors. Recent work in adversarial machine learning has shown that deep learning models are susceptible to gradient-based attacks, whereas non-differentiable models that report a score can be attacked by genetic algorithms that aim to systematically reduce the score. We propose a more general framework based on reinforcement learning (RL) for attacking static portable executable (PE) anti-malware engines. The general framework does not require a differentiable model nor does it require the engine to produce a score. Instead, an RL agent is equipped with a set of functionality-preserving operations that it may perform on the PE file. Through a series of games played against the anti-malware engine, it learns which sequences of operations are likely to result in evading the detector for any given malware sample. This enables completely black-box attacks against static PE anti-malware, and produces functional evasive malware samples as a direct result. We show in experiments that our method can attack a gradient-boosted machine learning model with evasion rates that are substantial and appear to be strongly dependent on the dataset. We demonstrate that attacks against this model appear to also evade components of publicly hosted antivirus engines. Adversarial training results are also presented: by retraining the model on evasive ransomware samples, a subsequent attack is 33\% less effective. However, there are overfitting dangers when adversarial training, which we note. We release code to allow researchers to reproduce and improve this approach.},
  pubstate = {prepublished},
  keywords = {9,Computer Science - Cryptography and Security},
  file = {C\:\\Users\\liuda\\Zotero\\storage\\KKW8XPNJ\\Anderson et al. - 2018 - Learning to Evade Static PE Machine Learning Malware Models via Reinforcement Learning.pdf;C\:\\Users\\liuda\\Zotero\\storage\\G9NYTFTV\\1801.html}
}

@inproceedings{castroAIMEDEvolvingMalware2019,
  title = {{{AIMED}}: {{Evolving Malware}} with {{Genetic Programming}} to {{Evade Detection}}},
  shorttitle = {{{AIMED}}},
  booktitle = {2019 18th {{IEEE International Conference On Trust}}, {{Security And Privacy In Computing And Communications}}/13th {{IEEE International Conference On Big Data Science And Engineering}} ({{TrustCom}}/{{BigDataSE}})},
  author = {Castro, Raphael Labaca and Schmitt, Corinna and Dreo, Gabi},
  date = {2019-08},
  pages = {240--247},
  issn = {2324-9013},
  doi = {10.1109/TrustCom/BigDataSE.2019.00040},
  url = {https://ieeexplore.ieee.org/abstract/document/8887384?casa_token=xCaErCjOiqsAAAAA:obBRvGXUzbZ_lSaOl1qdLXgo21emMsulFqJIuPRRKM9YW0EB8F8btGmDGzaRNRqaaebrbZk},
  urldate = {2024-09-23},
  abstract = {Genetic Programming (GP) has previously proved to achieve valuable results on the fields of image processing and arcade learning. Similarly, it can be used as an adversarial learning approach to evolve malware samples until static learning classifiers are no longer able to detect it. While the implementation is relatively simple compared with other Machine Learning approaches, results proved that GP can be a competitive solution to find adversarial malware examples comparing with similar methods. Thus, AIMED - Automatic Intelligent Malware Modifications to Evade Detection - was designed and imple-mented using genetic algorithms to evade malware classifiers. Our experiments suggest that the time to achieve adversarial malware samples can be reduced up to 50\% compared to classic random approaches. Moreover, we implemented AIMED to generate adversarial examples using individual malware scanners as target and tested the evasive files against further classifiers from both research and industry. The generated examples achieved up to 82\% of cross-evasion rates among the classifiers.},
  eventtitle = {2019 18th {{IEEE International Conference On Trust}}, {{Security And Privacy In Computing And Communications}}/13th {{IEEE International Conference On Big Data Science And Engineering}} ({{TrustCom}}/{{BigDataSE}})},
  keywords = {8,Adversarial Learning,AIMED,Genetic algorithms,Genetic Programming,Machine learning,Malware,Neural networks,Perturbation methods,Perturbations,Security,Sociology},
  file = {C\:\\Users\\liuda\\Zotero\\storage\\JIBUH36J\\Castro et al. - 2019 - AIMED Evolving Malware with Genetic Programming to Evade Detection.pdf;C\:\\Users\\liuda\\Zotero\\storage\\GYM3MMJ4\\8887384.html}
}

@online{chenInfoGANInterpretableRepresentation2016a,
  title = {{{InfoGAN}}: {{Interpretable Representation Learning}} by {{Information Maximizing Generative Adversarial Nets}}},
  shorttitle = {{{InfoGAN}}},
  author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  date = {2016-06-11},
  eprint = {1606.03657},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1606.03657},
  url = {http://arxiv.org/abs/1606.03657},
  urldate = {2024-10-07},
  abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.},
  pubstate = {prepublished},
  keywords = {12,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\liuda\\Zotero\\storage\\S8RYJULL\\Chen et al. - 2016 - InfoGAN Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.pdf;C\:\\Users\\liuda\\Zotero\\storage\\FQWWV3GI\\1606.html}
}

@article{demetrioAdversarialEXEmplesSurvey2021,
  title = {Adversarial {{EXEmples}}: {{A Survey}} and {{Experimental Evaluation}} of {{Practical Attacks}} on {{Machine Learning}} for {{Windows Malware Detection}}},
  shorttitle = {Adversarial {{EXEmples}}},
  author = {Demetrio, Luca and Coull, Scott E. and Biggio, Battista and Lagorio, Giovanni and Armando, Alessandro and Roli, Fabio},
  date = {2021-09-02},
  journaltitle = {ACM Trans. Priv. Secur.},
  volume = {24},
  number = {4},
  pages = {27:1--27:31},
  issn = {2471-2566},
  doi = {10.1145/3473039},
  url = {https://dl.acm.org/doi/10.1145/3473039},
  urldate = {2024-09-30},
  abstract = {Recent work has shown that adversarial Windows malware samples—referred to as adversarial EXEmples in this article—can bypass machine learning-based detection relying on static code analysis by perturbing relatively few input bytes. To preserve malicious functionality, previous attacks either add bytes to existing non-functional areas of the file, potentially limiting their effectiveness, or require running computationally demanding validation steps to discard malware variants that do not correctly execute in sandbox environments. In this work, we overcome these limitations by developing a unifying framework that does not only encompass and generalize previous attacks against machine-learning models, but also includes three novel attacks based on practical, functionality-preserving manipulations to the Windows Portable Executable file format. These attacks, named Full DOS, Extend, and Shift, inject the adversarial payload by respectively manipulating the DOS header, extending it, and shifting the content of the first section. Our experimental results show that these attacks outperform existing ones in both white-box and black-box scenarios, achieving a better tradeoff in terms of evasion rate and size of the injected payload, while also enabling evasion of models that have been shown to be robust to previous attacks. To facilitate reproducibility of our findings, we open source our framework and all the corresponding attack implementations as part of the secml-malware Python library. We conclude this work by discussing the limitations of current machine learning-based malware detectors, along with potential mitigation strategies based on embedding domain knowledge coming from subject-matter experts directly into the learning process.},
  keywords = {10},
  file = {C:\Users\liuda\Zotero\storage\LLWZBM8T\Demetrio et al. - 2021 - Adversarial EXEmples A Survey and Experimental Evaluation of Practical Attacks on Machine Learning.pdf}
}

@article{demetrioFunctionalityPreservingBlackBoxOptimization2021,
  title = {Functionality-{{Preserving Black-Box Optimization}} of {{Adversarial Windows Malware}}},
  author = {Demetrio, Luca and Biggio, Battista and Lagorio, Giovanni and Roli, Fabio and Armando, Alessandro},
  date = {2021},
  journaltitle = {IEEE Transactions on Information Forensics and Security},
  volume = {16},
  pages = {3469--3478},
  issn = {1556-6021},
  doi = {10.1109/TIFS.2021.3082330},
  url = {https://ieeexplore.ieee.org/abstract/document/9437194?casa_token=GKC1B14XIRIAAAAA:e98SzuUKfv04EGAE35MgdDBDN3bDl-e0m71LiS9Fg5ohK7kzwNK-Bbm7WsPMDSDxeNvttDY},
  urldate = {2024-10-14},
  abstract = {Windows malware detectors based on machine learning are vulnerable to adversarial examples, even if the attacker is only given black-box query access to the model. The main drawback of these attacks is that: ( i) they are query-inefficient, as they rely on iteratively applying random transformations to the input malware; and ( ii) they may also require executing the adversarial malware in a sandbox at each iteration of the optimization process, to ensure that its intrusive functionality is preserved. In this paper, we overcome these issues by presenting a novel family of black-box attacks that are both query-efficient and functionality-preserving, as they rely on the injection of benign content (which will never be executed) either at the end of the malicious file, or within some newly-created sections. Our attacks are formalized as a constrained minimization problem which also enables optimizing the trade-off between the probability of evading detection and the size of the injected payload. We empirically investigate this trade-off on two popular static Windows malware detectors, and show that our black-box attacks can bypass them with only few queries and small payloads, even when they only return the predicted labels. We also evaluate whether our attacks transfer to other commercial antivirus solutions, and surprisingly find that they can evade, on average, more than 12 commercial antivirus engines. We conclude by discussing the limitations of our approach, and its possible future extensions to target malware classifiers based on dynamic analysis.},
  eventtitle = {{{IEEE Transactions}} on {{Information Forensics}} and {{Security}}},
  keywords = {13,Adversarial examples,black-box optimization,Detectors,evasion attacks,Feature extraction,machine learning,Malware,malware detection,Minimization,Operating systems,Optimization,Payloads},
  file = {C\:\\Users\\liuda\\Zotero\\storage\\5ID4JCN8\\Demetrio et al. - 2021 - Functionality-Preserving Black-Box Optimization of Adversarial Windows Malware.pdf;C\:\\Users\\liuda\\Zotero\\storage\\PJZQ4WRP\\9437194.html}
}

@article{fangEvadingMalwareEngines2019,
  title = {Evading {{Anti-Malware Engines With Deep Reinforcement Learning}}},
  author = {Fang, Zhiyang and Wang, Junfeng and Li, Boya and Wu, Siqi and Zhou, Yingjie and Huang, Haiying},
  date = {2019},
  journaltitle = {IEEE Access},
  volume = {7},
  pages = {48867--48879},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2908033},
  url = {https://ieeexplore.ieee.org/abstract/document/8676031},
  urldate = {2024-09-18},
  abstract = {To reduce the risks of malicious software, malware detection methods using machine learning have received tremendous attention in recent years. Most of the conventional methods are based on supervised learning, which relies on static features with definite labels. However, recent studies have shown the models based on supervised learning are vulnerable to deliberate attacks. This work tends to expose and demonstrate the weakness in these models. A DQEAF framework using reinforcement learning to evade anti-malware engines is presented. DQEAF trains an AI agent through a neural network by constantly interacting with malware samples. Actions are a set of reasonable modifications, which do not damage samples’ structure and functions. The agent selects the optimal sequence of actions to modify the malware samples, thus they can bypass the detection engines. The training process depends on the characteristics of the raw binary stream features of samples. The experiments show that the proposed method has a success rate of 75\%. The efficacy of the proposed DQEAF has also been evaluated by other families of malicious software, which shows good robustness.},
  eventtitle = {{{IEEE Access}}},
  keywords = {6,Anti-malware engines evasion,deep machine learning,Engines,Feature extraction,Malware,malware detection,reinforcement learning,Reinforcement learning,Supervised learning,Training},
  file = {C\:\\Users\\liuda\\Zotero\\storage\\D6FBLB6C\\Fang et al. - 2019 - Evading Anti-Malware Engines With Deep Reinforcement Learning.pdf;C\:\\Users\\liuda\\Zotero\\storage\\GGPSERMW\\8676031.html}
}

@online{huGeneratingAdversarialMalware2017,
  title = {Generating {{Adversarial Malware Examples}} for {{Black-Box Attacks Based}} on {{GAN}}},
  author = {Hu, Weiwei and Tan, Ying},
  date = {2017-02-20},
  eprint = {1702.05983},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1702.05983},
  urldate = {2024-09-18},
  abstract = {Machine learning has been used to detect new malware in recent years, while malware authors have strong motivation to attack such algorithms.Malware authors usually have no access to the detailed structures and parameters of the machine learning models used by malware detection systems, and therefore they can only perform black-box attacks. This paper proposes a generative adversarial network (GAN) based algorithm named MalGAN to generate adversarial malware examples, which are able to bypass black-box machine learning based detection models. MalGAN uses a substitute detector to fit the black-box malware detection system. A generative network is trained to minimize the generated adversarial examples’ malicious probabilities predicted by the substitute detector. The superiority of MalGAN over traditional gradient based adversarial example generation algorithms is that MalGAN is able to decrease the detection rate to nearly zero and make the retraining based defensive method against adversarial examples hard to work.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {5,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {C:\Users\liuda\Zotero\storage\W36MX2CP\Hu and Tan - 2017 - Generating Adversarial Malware Examples for Black-Box Attacks Based on GAN.pdf}
}

@inproceedings{linIDSGANGenerativeAdversarial2022,
  title = {{{IDSGAN}}: {{Generative Adversarial Networks}} for~{{Attack Generation Against Intrusion Detection}}},
  shorttitle = {{{IDSGAN}}},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Lin, Zilong and Shi, Yong and Xue, Zhi},
  editor = {Gama, João and Li, Tianrui and Yu, Yang and Chen, Enhong and Zheng, Yu and Teng, Fei},
  date = {2022},
  pages = {79--91},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-031-05981-0_7},
  abstract = {As an essential tool in security, the intrusion detection system bears the responsibility of the defense to network attacks performed by malicious traffic. Nowadays, with the help of machine learning algorithms, intrusion detection systems develop rapidly. However, the robustness of this system is questionable when it faces adversarial attacks. For the robustness of detection systems, more potential attack approaches are under research. In this paper, a framework of the generative adversarial networks, called IDSGAN, is proposed to generate the adversarial malicious traffic records aiming to attack intrusion detection systems by deceiving and evading the detection. Given that the internal structure and parameters of the detection system are unknown to attackers, the adversarial attack examples perform the black-box attacks against the detection system. IDSGAN leverages a generator to transform original malicious traffic records into adversarial malicious ones. A discriminator classifies traffic examples and dynamically learns the real-time black-box detection system. More significantly, the restricted modification mechanism is designed for the adversarial generation to preserve original attack functionalities of adversarial traffic records. The effectiveness of the model is indicated by attacking multiple algorithm-based detection models with different attack categories. The robustness is verified by changing the number of the modified features. A comparative experiment with adversarial attack baselines demonstrates the superiority of our model.},
  isbn = {978-3-031-05981-0},
  langid = {english},
  keywords = {15,Adversarial examples,Black-box attack,Generative adversarial networks,Intrusion detection},
  file = {C:\Users\liuda\Zotero\storage\EUA57JAS\Lin et al. - 2022 - IDSGAN Generative Adversarial Networks for Attack Generation Against Intrusion Detection.pdf}
}

@online{MalwareStatisticsTrends,
  title = {Malware {{Statistics}} \& {{Trends Report}} | {{AV-TEST}}},
  url = {https://www.av-test.org/en/statistics/malware/},
  urldate = {2024-11-01},
  keywords = {numbers}
}

@article{nguyenGenerativeAdversarialNetworks2023,
  title = {Generative Adversarial Networks and Image-Based Malware Classification},
  author = {Nguyen, Huy and Di Troia, Fabio and Ishigaki, Genya and Stamp, Mark},
  date = {2023-11-01},
  journaltitle = {Journal of Computer Virology and Hacking Techniques},
  shortjournal = {J Comput Virol Hack Tech},
  volume = {19},
  number = {4},
  pages = {579--595},
  issn = {2263-8733},
  doi = {10.1007/s11416-023-00465-2},
  url = {https://doi.org/10.1007/s11416-023-00465-2},
  urldate = {2024-09-15},
  abstract = {For efficient malware removal, determination of malware threat levels, and damage estimation, malware family classification plays a critical role. In this paper, we extract features from malware executable files and represent them as images using various approaches. We then focus on generative adversarial networks (GAN) for multiclass classification and compare our GAN results to other popular machine learning techniques, including support vector machine (SVM), XGBoost, and restricted Boltzmann machines (RBM). We find that the AC-GAN discriminator is generally competitive with other machine learning techniques. We also evaluate the utility of the GAN generative model for adversarial attacks on image-based malware detection. While AC-GAN generated images are visually impressive, we find that they are easily distinguished from real malware images using any of several learning techniques. This result indicates that our GAN generated images are of surprisingly little value in adversarial attacks.},
  langid = {english},
  keywords = {1,Artificial Intelligence},
  file = {C:\Users\liuda\Zotero\storage\ADKB6C5V\Nguyen et al. - 2023 - Generative adversarial networks and image-based malware classification.pdf}
}

@inproceedings{rosenbergGenericBlackBoxEndEnd2018,
  title = {Generic {{Black-Box End-to-End Attack Against State}} of the {{Art API Call Based Malware Classifiers}}},
  booktitle = {Research in {{Attacks}}, {{Intrusions}}, and {{Defenses}}},
  author = {Rosenberg, Ishai and Shabtai, Asaf and Rokach, Lior and Elovici, Yuval},
  editor = {Bailey, Michael and Holz, Thorsten and Stamatogiannakis, Manolis and Ioannidis, Sotiris},
  date = {2018},
  pages = {490--510},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-00470-5_23},
  abstract = {In this paper, we present a black-box attack against API call based machine learning malware classifiers, focusing on generating adversarial sequences combining API calls and static features (e.g., printable strings) that will be misclassified by the classifier without affecting the malware functionality. We show that this attack is effective against many classifiers due to the transferability principle between RNN variants, feed forward DNNs, and traditional machine learning classifiers such as SVM. We also implement GADGET, a software framework to convert any malware binary to a binary undetected by malware classifiers, using the proposed attack, without access to the malware source code.},
  isbn = {978-3-030-00470-5},
  langid = {english},
  keywords = {14},
  file = {C:\Users\liuda\Zotero\storage\P3HNKHZV\Rosenberg et al. - 2018 - Generic Black-Box End-to-End Attack Against State of the Art API Call Based Malware Classifiers.pdf}
}

@inproceedings{saxeDeepNeuralNetwork2015,
  title = {Deep Neural Network Based Malware Detection Using Two Dimensional Binary Program Features},
  booktitle = {2015 10th {{International Conference}} on {{Malicious}} and {{Unwanted Software}} ({{MALWARE}})},
  author = {Saxe, Joshua and Berlin, Konstantin},
  date = {2015-10},
  pages = {11--20},
  doi = {10.1109/MALWARE.2015.7413680},
  url = {https://ieeexplore.ieee.org/abstract/document/7413680},
  urldate = {2024-11-04},
  abstract = {In this paper we introduce a deep neural network based malware detection system that Invincea has developed, which achieves a usable detection rate at an extremely low false positive rate and scales to real world training example volumes on commodity hardware. We show that our system achieves a 95\% detection rate at 0.1\% false positive rate (FPR), based on more than 400,000 software binaries sourced directly from our customers and internal malware databases. In addition, we describe a non-parametric method for adjusting the classifier's scores to better represent expected precision in the deployment environment. Our results demonstrate that it is now feasible to quickly train and deploy a low resource, highly accurate machine learning classification model, with false positive rates that approach traditional labor intensive expert rule based malware detection, while also detecting previously unseen malware missed by these traditional approaches. Since machine learning models tend to improve with larger datasizes, we foresee deep neural network classification models gaining in importance as part of a layered network defense strategy in coming years.},
  eventtitle = {2015 10th {{International Conference}} on {{Malicious}} and {{Unwanted Software}} ({{MALWARE}})},
  keywords = {Computational modeling,Data models,Feature extraction,Histograms,Malware,Neural networks,Training},
  file = {C\:\\Users\\liuda\\Zotero\\storage\\453R48Q4\\Saxe and Berlin - 2015 - Deep neural network based malware detection using two dimensional binary program features.pdf;C\:\\Users\\liuda\\Zotero\\storage\\2TM9WUM2\\7413680.html}
}

@article{yusteOptimizationCodeCaves2022,
  title = {Optimization of Code Caves in Malware Binaries to Evade Machine Learning Detectors},
  author = {Yuste, Javier and Pardo, Eduardo G. and Tapiador, Juan},
  date = {2022-05-01},
  journaltitle = {Computers \& Security},
  shortjournal = {Computers \& Security},
  volume = {116},
  pages = {102643},
  issn = {0167-4048},
  doi = {10.1016/j.cose.2022.102643},
  url = {https://www.sciencedirect.com/science/article/pii/S0167404822000426},
  urldate = {2024-10-07},
  abstract = {Machine Learning (ML) techniques, especially Artificial Neural Networks, have been widely adopted as a tool for malware detection due to their high accuracy when classifying programs as benign or malicious. However, these techniques are vulnerable to Adversarial Examples (AEs), i.e., carefully crafted samples designed by an attacker to be misclassified by the target model. In this work, we propose a general method to produce AEs from existing malware, which is useful to increase the robustness of ML-based models. Our method dynamically introduces unused blocks (caves) in malware binaries, preserving their original functionality. Then, by using optimization techniques based on Genetic Algorithms, we determine the most adequate content to place in such code caves to achieve misclassification. We evaluate our model in a black-box setting with a well-known state-of-the-art architecture (MalConv), resulting in a successful evasion rate of 97.99~\% from the 2k tested malware samples. Additionally, we successfully test the transferability of our proposal to commercial AV engines available at VirusTotal, showing a reduction in the detection rate for the crafted AEs. Finally, the obtained AEs are used to retrain the ML-based malware detector previously evaluated, showing an improve on its robustness.},
  keywords = {11,Adversarial example,Evasion,Genetic algorithm,Machine learning,Malware},
  file = {C\:\\Users\\liuda\\Zotero\\storage\\F6ZYIGEX\\Yuste et al. - 2022 - Optimization of code caves in malware binaries to evade machine learning detectors.pdf;C\:\\Users\\liuda\\Zotero\\storage\\DTNMTQT4\\S0167404822000426.html}
}

@article{zhongMalFoxCamouflagedAdversarial2024,
  title = {{{MalFox}}: {{Camouflaged Adversarial Malware Example Generation Based}} on {{Conv-GANs Against Black-Box Detectors}}},
  shorttitle = {{{MalFox}}},
  author = {Zhong, Fangtian and Cheng, Xiuzhen and Yu, Dongxiao and Gong, Bei and Song, Shuaiwen and Yu, Jiguo},
  date = {2024-04},
  journaltitle = {IEEE Transactions on Computers},
  volume = {73},
  number = {4},
  pages = {980--993},
  issn = {1557-9956},
  doi = {10.1109/TC.2023.3236901},
  url = {https://ieeexplore.ieee.org/abstract/document/10017127?casa_token=tILgWcHIR8cAAAAA:lqMBa9dcRq7juZAQV_fBQij4ZQYyD5NaBbYLfnzrfnbcr_j64SHELMdyIoHmO-F6nsRgZtU},
  urldate = {2024-09-15},
  abstract = {Deep learning is a thriving field currently stuffed with many practical applications and active research topics. It allows computers to learn from experience and to understand the world in terms of a hierarchy of concepts, with each being defined through its relations to simpler concepts. Relying on the strong capabilities of deep learning, we propose a convolutional generative adversarial network-based (Conv-GAN) framework titled MalFox, targeting adversarial malware example generation against third-party black-box malware detectors. Motivated by the rival game between malware authors and malware detectors, MalFox adopts a confrontational approach to produce perturbation paths, with each formed by up to three methods (namely Obfusmal, Stealmal, and Hollowmal) to generate adversarial malware examples. To demonstrate the effectiveness of MalFox, we collect a large dataset consisting of both malware and benignware programs, and investigate the performance of MalFox in terms of accuracy, detection rate, and evasive rate of the generated adversarial malware examples. Our evaluation indicates that the accuracy can be as high as 99.0\% which significantly outperforms the other 12 well-known learning models. Furthermore, the detection rate is dramatically decreased by 56.8\% on average, and the average evasive rate is noticeably improved by up to 56.2\%.},
  eventtitle = {{{IEEE Transactions}} on {{Computers}}},
  keywords = {2,Adversarial malware examples,Closed box,Computer viruses,deep learning,Detectors,Electronic mail,Engines,generative adversarial network,malware,Malware,Perturbation methods},
  file = {C\:\\Users\\liuda\\Zotero\\storage\\3UTSNQ46\\Zhong et al. - 2024 - MalFox Camouflaged Adversarial Malware Example Generation Based on Conv-GANs Against Black-Box Dete.pdf;C\:\\Users\\liuda\\Zotero\\storage\\7PV5GAG2\\10017127.html}
}

@article{zhongReinforcementLearningBased2022,
  title = {Reinforcement Learning Based Adversarial Malware Example Generation against Black-Box Detectors},
  author = {Zhong, Fangtian and Hu, Pengfei and Zhang, Guoming and Li, Hong and Cheng, Xiuzhen},
  date = {2022-10-01},
  journaltitle = {Computers \& Security},
  shortjournal = {Computers \& Security},
  volume = {121},
  pages = {102869},
  issn = {0167-4048},
  doi = {10.1016/j.cose.2022.102869},
  url = {https://www.sciencedirect.com/science/article/pii/S0167404822002632},
  urldate = {2024-09-14},
  abstract = {Recent advances in machine learning offer attractive tools for sophisticated adversaries. An attacker could transform malware into its adversarial version but retain its malicious functionality by employing a dedicated perturbation method. These adversarial malware examples have demonstrated the effectiveness to bypass antivirus engines. However, recent works only leverage a single perturbation method to generate adversarial examples, which cannot defeat advanced detectors. In this paper, we propose a reinforcement learning-based framework called MalInfo, which could generate powerful adversarial malware examples to evade the third-party detectors via an adaptive selection of a perturbation path for each malware in our collected dataset with 1000 diverse malware. To cope with limited computation, MalInfo applies either dynamic programming or temporal difference learning to choose the optimal perturbation path where each path is formed by the combination of Obfusmal, Stealmal, and Hollowmal. We provide a proof-of-concept implementation and extensive evaluation of our framework. Both the detection rate and evasive rate have substantially been improved compared with the state-of-art research MalFox Zhong et~al. (2021). To be specific, The average detection rates for dynamic programming and temporal difference learning are 23.2\% (21.9\% lower than MalFox) and 27.5\% (7.4\% lower than MalFox), respectively, and the average evasive rates are 65.8\% (17.1\% higher than MalFox) and 59.4\% (5.7\% higher than MalFox), respectively.},
  keywords = {3,Adversarial malware examples,Dynamic programming,Malware,Reinforcement learning,Temporal difference learning},
  file = {C\:\\Users\\liuda\\Zotero\\storage\\8YXYHIMR\\Zhong et al. - 2022 - Reinforcement learning based adversarial malware example generation against black-box detectors.pdf;C\:\\Users\\liuda\\Zotero\\storage\\S3H8ZSE4\\S0167404822002632.html}
}

@article{zhuNgramMalGANEvading2022,
  title = {N-Gram {{MalGAN}}: {{Evading}} Machine Learning Detection via Feature n-Gram},
  shorttitle = {N-Gram {{MalGAN}}},
  author = {Zhu, Enmin and Zhang, Jianjie and Yan, Jijie and Chen, Kongyang and Gao, Chongzhi},
  date = {2022-08-01},
  journaltitle = {Digital Communications and Networks},
  shortjournal = {Digital Communications and Networks},
  volume = {8},
  number = {4},
  pages = {485--491},
  issn = {2352-8648},
  doi = {10.1016/j.dcan.2021.11.007},
  url = {https://www.sciencedirect.com/science/article/pii/S2352864821000973},
  urldate = {2024-09-23},
  abstract = {In recent years, many adversarial malware examples with different feature strategies, especially GAN and its variants, have been introduced to handle the security threats, e.g., evading the detection of machine learning detectors. However, these solutions still suffer from problems of complicated deployment or long running time. In this paper, we propose an n-gram MalGAN method to solve these problems. We borrow the idea of n-gram from the Natural Language Processing (NLP) area to expand feature sources for adversarial malware examples in MalGAN. Generally, the n-gram MalGAN obtains the feature vector directly from the hexadecimal bytecodes of the executable file. It can be implemented easily and conveniently with a simple program language (e.g., C++), with no need for any prior knowledge of the executable file or any professional feature extraction tools. These features are functionally independent and thus can be added to the non-functional area of the malicious program to maintain its original executability. In this way, the n-gram could make the adversarial attack easier and more convenient. Experimental results show that the evasion rate of the n-gram MalGAN is at least 88.58\% to attack different machine learning algorithms under an appropriate group rate, growing to even 100\% for the Random Forest algorithm.},
  keywords = {7,Adversarial examples,Machine learning,MalGAN,N-gram},
  file = {C\:\\Users\\liuda\\Zotero\\storage\\7SHKB5ND\\Zhu et al. - 2022 - N-gram MalGAN Evading machine learning detection via feature n-gram.pdf;C\:\\Users\\liuda\\Zotero\\storage\\IH726JGY\\S2352864821000973.html}
}
