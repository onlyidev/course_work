@online{andersonLearningEvadeStatic2018,
  title = {Learning to {{Evade Static PE Machine Learning Malware Models}} via {{Reinforcement Learning}}},
  author = {Anderson, Hyrum S. and Kharkar, Anant and Filar, Bobby and Evans, David and Roth, Phil},
  date = {2018-01-30},
  eprint = {1801.08917},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1801.08917},
  url = {http://arxiv.org/abs/1801.08917},
  urldate = {2024-09-30},
  abstract = {Machine learning is a popular approach to signatureless malware detection because it can generalize to never-before-seen malware families and polymorphic strains. This has resulted in its practical use for either primary detection engines or for supplementary heuristic detection by anti-malware vendors. Recent work in adversarial machine learning has shown that deep learning models are susceptible to gradient-based attacks, whereas non-differentiable models that report a score can be attacked by genetic algorithms that aim to systematically reduce the score. We propose a more general framework based on reinforcement learning (RL) for attacking static portable executable (PE) anti-malware engines. The general framework does not require a differentiable model nor does it require the engine to produce a score. Instead, an RL agent is equipped with a set of functionality-preserving operations that it may perform on the PE file. Through a series of games played against the anti-malware engine, it learns which sequences of operations are likely to result in evading the detector for any given malware sample. This enables completely black-box attacks against static PE anti-malware, and produces functional evasive malware samples as a direct result. We show in experiments that our method can attack a gradient-boosted machine learning model with evasion rates that are substantial and appear to be strongly dependent on the dataset. We demonstrate that attacks against this model appear to also evade components of publicly hosted antivirus engines. Adversarial training results are also presented: by retraining the model on evasive ransomware samples, a subsequent attack is 33\% less effective. However, there are overfitting dangers when adversarial training, which we note. We release code to allow researchers to reproduce and improve this approach.},
  pubstate = {prepublished},
  keywords = {9,Computer Science - Cryptography and Security},
  file = {C\:\\Users\\liuda\\Zotero\\storage\\KKW8XPNJ\\Anderson et al. - 2018 - Learning to Evade Static PE Machine Learning Malware Models via Reinforcement Learning.pdf;C\:\\Users\\liuda\\Zotero\\storage\\G9NYTFTV\\1801.html}
}

@inproceedings{castroAIMEDEvolvingMalware2019,
  title = {{{AIMED}}: {{Evolving Malware}} with {{Genetic Programming}} to {{Evade Detection}}},
  shorttitle = {{{AIMED}}},
  booktitle = {2019 18th {{IEEE International Conference On Trust}}, {{Security And Privacy In Computing And Communications}}/13th {{IEEE International Conference On Big Data Science And Engineering}} ({{TrustCom}}/{{BigDataSE}})},
  author = {Castro, Raphael Labaca and Schmitt, Corinna and Dreo, Gabi},
  date = {2019-08},
  pages = {240--247},
  issn = {2324-9013},
  doi = {10.1109/TrustCom/BigDataSE.2019.00040},
  url = {https://ieeexplore.ieee.org/abstract/document/8887384?casa_token=xCaErCjOiqsAAAAA:obBRvGXUzbZ_lSaOl1qdLXgo21emMsulFqJIuPRRKM9YW0EB8F8btGmDGzaRNRqaaebrbZk},
  urldate = {2024-09-23},
  abstract = {Genetic Programming (GP) has previously proved to achieve valuable results on the fields of image processing and arcade learning. Similarly, it can be used as an adversarial learning approach to evolve malware samples until static learning classifiers are no longer able to detect it. While the implementation is relatively simple compared with other Machine Learning approaches, results proved that GP can be a competitive solution to find adversarial malware examples comparing with similar methods. Thus, AIMED - Automatic Intelligent Malware Modifications to Evade Detection - was designed and imple-mented using genetic algorithms to evade malware classifiers. Our experiments suggest that the time to achieve adversarial malware samples can be reduced up to 50\% compared to classic random approaches. Moreover, we implemented AIMED to generate adversarial examples using individual malware scanners as target and tested the evasive files against further classifiers from both research and industry. The generated examples achieved up to 82\% of cross-evasion rates among the classifiers.},
  eventtitle = {2019 18th {{IEEE International Conference On Trust}}, {{Security And Privacy In Computing And Communications}}/13th {{IEEE International Conference On Big Data Science And Engineering}} ({{TrustCom}}/{{BigDataSE}})},
  keywords = {8,Adversarial Learning,AIMED,Genetic algorithms,Genetic Programming,Machine learning,Malware,Neural networks,Perturbation methods,Perturbations,Security,Sociology},
  file = {C\:\\Users\\liuda\\Zotero\\storage\\JIBUH36J\\Castro et al. - 2019 - AIMED Evolving Malware with Genetic Programming to Evade Detection.pdf;C\:\\Users\\liuda\\Zotero\\storage\\GYM3MMJ4\\8887384.html}
}

@online{chenInfoGANInterpretableRepresentation2016a,
  title = {{{InfoGAN}}: {{Interpretable Representation Learning}} by {{Information Maximizing Generative Adversarial Nets}}},
  shorttitle = {{{InfoGAN}}},
  author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  date = {2016-06-11},
  eprint = {1606.03657},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1606.03657},
  url = {http://arxiv.org/abs/1606.03657},
  urldate = {2024-10-07},
  abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.},
  pubstate = {prepublished},
  keywords = {12,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\liuda\\Zotero\\storage\\S8RYJULL\\Chen et al. - 2016 - InfoGAN Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.pdf;C\:\\Users\\liuda\\Zotero\\storage\\FQWWV3GI\\1606.html}
}

@article{demetrioAdversarialEXEmplesSurvey2021,
  title = {Adversarial {{EXEmples}}: {{A Survey}} and {{Experimental Evaluation}} of {{Practical Attacks}} on {{Machine Learning}} for {{Windows Malware Detection}}},
  shorttitle = {Adversarial {{EXEmples}}},
  author = {Demetrio, Luca and Coull, Scott E. and Biggio, Battista and Lagorio, Giovanni and Armando, Alessandro and Roli, Fabio},
  date = {2021-09-02},
  journaltitle = {ACM Trans. Priv. Secur.},
  volume = {24},
  number = {4},
  pages = {27:1--27:31},
  issn = {2471-2566},
  doi = {10.1145/3473039},
  url = {https://dl.acm.org/doi/10.1145/3473039},
  urldate = {2024-09-30},
  abstract = {Recent work has shown that adversarial Windows malware samples—referred to as adversarial EXEmples in this article—can bypass machine learning-based detection relying on static code analysis by perturbing relatively few input bytes. To preserve malicious functionality, previous attacks either add bytes to existing non-functional areas of the file, potentially limiting their effectiveness, or require running computationally demanding validation steps to discard malware variants that do not correctly execute in sandbox environments. In this work, we overcome these limitations by developing a unifying framework that does not only encompass and generalize previous attacks against machine-learning models, but also includes three novel attacks based on practical, functionality-preserving manipulations to the Windows Portable Executable file format. These attacks, named Full DOS, Extend, and Shift, inject the adversarial payload by respectively manipulating the DOS header, extending it, and shifting the content of the first section. Our experimental results show that these attacks outperform existing ones in both white-box and black-box scenarios, achieving a better tradeoff in terms of evasion rate and size of the injected payload, while also enabling evasion of models that have been shown to be robust to previous attacks. To facilitate reproducibility of our findings, we open source our framework and all the corresponding attack implementations as part of the secml-malware Python library. We conclude this work by discussing the limitations of current machine learning-based malware detectors, along with potential mitigation strategies based on embedding domain knowledge coming from subject-matter experts directly into the learning process.},
  keywords = {10},
  file = {C:\Users\liuda\Zotero\storage\LLWZBM8T\Demetrio et al. - 2021 - Adversarial EXEmples A Survey and Experimental Evaluation of Practical Attacks on Machine Learning.pdf}
}

@article{fangEvadingMalwareEngines2019,
  title = {Evading {{Anti-Malware Engines With Deep Reinforcement Learning}}},
  author = {Fang, Zhiyang and Wang, Junfeng and Li, Boya and Wu, Siqi and Zhou, Yingjie and Huang, Haiying},
  date = {2019},
  journaltitle = {IEEE Access},
  volume = {7},
  pages = {48867--48879},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2908033},
  url = {https://ieeexplore.ieee.org/abstract/document/8676031},
  urldate = {2024-09-18},
  abstract = {To reduce the risks of malicious software, malware detection methods using machine learning have received tremendous attention in recent years. Most of the conventional methods are based on supervised learning, which relies on static features with definite labels. However, recent studies have shown the models based on supervised learning are vulnerable to deliberate attacks. This work tends to expose and demonstrate the weakness in these models. A DQEAF framework using reinforcement learning to evade anti-malware engines is presented. DQEAF trains an AI agent through a neural network by constantly interacting with malware samples. Actions are a set of reasonable modifications, which do not damage samples’ structure and functions. The agent selects the optimal sequence of actions to modify the malware samples, thus they can bypass the detection engines. The training process depends on the characteristics of the raw binary stream features of samples. The experiments show that the proposed method has a success rate of 75\%. The efficacy of the proposed DQEAF has also been evaluated by other families of malicious software, which shows good robustness.},
  eventtitle = {{{IEEE Access}}},
  keywords = {6,Anti-malware engines evasion,deep machine learning,Engines,Feature extraction,Malware,malware detection,reinforcement learning,Reinforcement learning,Supervised learning,Training},
  file = {C\:\\Users\\liuda\\Zotero\\storage\\D6FBLB6C\\Fang et al. - 2019 - Evading Anti-Malware Engines With Deep Reinforcement Learning.pdf;C\:\\Users\\liuda\\Zotero\\storage\\GGPSERMW\\8676031.html}
}

@online{huGeneratingAdversarialMalware2017,
  title = {Generating {{Adversarial Malware Examples}} for {{Black-Box Attacks Based}} on {{GAN}}},
  author = {Hu, Weiwei and Tan, Ying},
  date = {2017-02-20},
  eprint = {1702.05983},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1702.05983},
  urldate = {2024-09-18},
  abstract = {Machine learning has been used to detect new malware in recent years, while malware authors have strong motivation to attack such algorithms.Malware authors usually have no access to the detailed structures and parameters of the machine learning models used by malware detection systems, and therefore they can only perform black-box attacks. This paper proposes a generative adversarial network (GAN) based algorithm named MalGAN to generate adversarial malware examples, which are able to bypass black-box machine learning based detection models. MalGAN uses a substitute detector to fit the black-box malware detection system. A generative network is trained to minimize the generated adversarial examples’ malicious probabilities predicted by the substitute detector. The superiority of MalGAN over traditional gradient based adversarial example generation algorithms is that MalGAN is able to decrease the detection rate to nearly zero and make the retraining based defensive method against adversarial examples hard to work.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {5,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {C:\Users\liuda\Zotero\storage\W36MX2CP\Hu and Tan - 2017 - Generating Adversarial Malware Examples for Black-Box Attacks Based on GAN.pdf}
}

@inproceedings{kawaiImprovedMalGANAvoiding2019,
  title = {Improved {{MalGAN}}: {{Avoiding Malware Detector}} by {{Leaning Cleanware Features}}},
  shorttitle = {Improved {{MalGAN}}},
  booktitle = {2019 {{International Conference}} on {{Artificial Intelligence}} in {{Information}} and {{Communication}} ({{ICAIIC}})},
  author = {Kawai, Masataka and Ota, Kaoru and Dong, Mianxing},
  date = {2019-02},
  pages = {040--045},
  doi = {10.1109/ICAIIC.2019.8669079},
  url = {https://ieeexplore.ieee.org/abstract/document/8669079},
  urldate = {2024-09-14},
  abstract = {In recent years, researches on malware detection using machine learning have been attracting wide attention. At the same time, how to avoid these detections is also regarded as an emerging topic. In this paper, we focus on the avoidance of malware detection based on Generative Adversarial Network (GAN). Previous GAN-based researches use the same feature quantities for learning malware detection. Moreover, existing learning algorithms use multiple malware, which affects the performance of avoidance and is not realistic on attackers. To settle this issue, we apply differentiated learning methods with the different feature quantities and only one malware. Experimental results show that our method can achieve better performance than existing ones.},
  eventtitle = {2019 {{International Conference}} on {{Artificial Intelligence}} in {{Information}} and {{Communication}} ({{ICAIIC}})},
  keywords = {4,adversarial machine learning,deep learning,Detectors,Feature extraction,Gallium nitride,GAN,Generators,Malware,malware detection,Radio frequency,Training data},
  file = {C:\Users\liuda\Zotero\storage\48JT6ER7\8669079.html}
}

@article{nguyenGenerativeAdversarialNetworks2023,
  title = {Generative Adversarial Networks and Image-Based Malware Classification},
  author = {Nguyen, Huy and Di Troia, Fabio and Ishigaki, Genya and Stamp, Mark},
  date = {2023-11-01},
  journaltitle = {Journal of Computer Virology and Hacking Techniques},
  shortjournal = {J Comput Virol Hack Tech},
  volume = {19},
  number = {4},
  pages = {579--595},
  issn = {2263-8733},
  doi = {10.1007/s11416-023-00465-2},
  url = {https://doi.org/10.1007/s11416-023-00465-2},
  urldate = {2024-09-15},
  abstract = {For efficient malware removal, determination of malware threat levels, and damage estimation, malware family classification plays a critical role. In this paper, we extract features from malware executable files and represent them as images using various approaches. We then focus on generative adversarial networks (GAN) for multiclass classification and compare our GAN results to other popular machine learning techniques, including support vector machine (SVM), XGBoost, and restricted Boltzmann machines (RBM). We find that the AC-GAN discriminator is generally competitive with other machine learning techniques. We also evaluate the utility of the GAN generative model for adversarial attacks on image-based malware detection. While AC-GAN generated images are visually impressive, we find that they are easily distinguished from real malware images using any of several learning techniques. This result indicates that our GAN generated images are of surprisingly little value in adversarial attacks.},
  langid = {english},
  keywords = {1,Artificial Intelligence},
  file = {C:\Users\liuda\Zotero\storage\ADKB6C5V\Nguyen et al. - 2023 - Generative adversarial networks and image-based malware classification.pdf}
}

@article{yusteOptimizationCodeCaves2022,
  title = {Optimization of Code Caves in Malware Binaries to Evade Machine Learning Detectors},
  author = {Yuste, Javier and Pardo, Eduardo G. and Tapiador, Juan},
  date = {2022-05-01},
  journaltitle = {Computers \& Security},
  shortjournal = {Computers \& Security},
  volume = {116},
  pages = {102643},
  issn = {0167-4048},
  doi = {10.1016/j.cose.2022.102643},
  url = {https://www.sciencedirect.com/science/article/pii/S0167404822000426},
  urldate = {2024-10-07},
  abstract = {Machine Learning (ML) techniques, especially Artificial Neural Networks, have been widely adopted as a tool for malware detection due to their high accuracy when classifying programs as benign or malicious. However, these techniques are vulnerable to Adversarial Examples (AEs), i.e., carefully crafted samples designed by an attacker to be misclassified by the target model. In this work, we propose a general method to produce AEs from existing malware, which is useful to increase the robustness of ML-based models. Our method dynamically introduces unused blocks (caves) in malware binaries, preserving their original functionality. Then, by using optimization techniques based on Genetic Algorithms, we determine the most adequate content to place in such code caves to achieve misclassification. We evaluate our model in a black-box setting with a well-known state-of-the-art architecture (MalConv), resulting in a successful evasion rate of 97.99~\% from the 2k tested malware samples. Additionally, we successfully test the transferability of our proposal to commercial AV engines available at VirusTotal, showing a reduction in the detection rate for the crafted AEs. Finally, the obtained AEs are used to retrain the ML-based malware detector previously evaluated, showing an improve on its robustness.},
  keywords = {11,Adversarial example,Evasion,Genetic algorithm,Machine learning,Malware},
  file = {C\:\\Users\\liuda\\Zotero\\storage\\F6ZYIGEX\\Yuste et al. - 2022 - Optimization of code caves in malware binaries to evade machine learning detectors.pdf;C\:\\Users\\liuda\\Zotero\\storage\\DTNMTQT4\\S0167404822000426.html}
}

@article{zhongMalFoxCamouflagedAdversarial2024,
  title = {{{MalFox}}: {{Camouflaged Adversarial Malware Example Generation Based}} on {{Conv-GANs Against Black-Box Detectors}}},
  shorttitle = {{{MalFox}}},
  author = {Zhong, Fangtian and Cheng, Xiuzhen and Yu, Dongxiao and Gong, Bei and Song, Shuaiwen and Yu, Jiguo},
  date = {2024-04},
  journaltitle = {IEEE Transactions on Computers},
  volume = {73},
  number = {4},
  pages = {980--993},
  issn = {1557-9956},
  doi = {10.1109/TC.2023.3236901},
  url = {https://ieeexplore.ieee.org/abstract/document/10017127?casa_token=tILgWcHIR8cAAAAA:lqMBa9dcRq7juZAQV_fBQij4ZQYyD5NaBbYLfnzrfnbcr_j64SHELMdyIoHmO-F6nsRgZtU},
  urldate = {2024-09-15},
  abstract = {Deep learning is a thriving field currently stuffed with many practical applications and active research topics. It allows computers to learn from experience and to understand the world in terms of a hierarchy of concepts, with each being defined through its relations to simpler concepts. Relying on the strong capabilities of deep learning, we propose a convolutional generative adversarial network-based (Conv-GAN) framework titled MalFox, targeting adversarial malware example generation against third-party black-box malware detectors. Motivated by the rival game between malware authors and malware detectors, MalFox adopts a confrontational approach to produce perturbation paths, with each formed by up to three methods (namely Obfusmal, Stealmal, and Hollowmal) to generate adversarial malware examples. To demonstrate the effectiveness of MalFox, we collect a large dataset consisting of both malware and benignware programs, and investigate the performance of MalFox in terms of accuracy, detection rate, and evasive rate of the generated adversarial malware examples. Our evaluation indicates that the accuracy can be as high as 99.0\% which significantly outperforms the other 12 well-known learning models. Furthermore, the detection rate is dramatically decreased by 56.8\% on average, and the average evasive rate is noticeably improved by up to 56.2\%.},
  eventtitle = {{{IEEE Transactions}} on {{Computers}}},
  keywords = {2,Adversarial malware examples,Closed box,Computer viruses,deep learning,Detectors,Electronic mail,Engines,generative adversarial network,malware,Malware,Perturbation methods},
  file = {C\:\\Users\\liuda\\Zotero\\storage\\3UTSNQ46\\Zhong et al. - 2024 - MalFox Camouflaged Adversarial Malware Example Generation Based on Conv-GANs Against Black-Box Dete.pdf;C\:\\Users\\liuda\\Zotero\\storage\\7PV5GAG2\\10017127.html}
}

@article{zhongReinforcementLearningBased2022,
  title = {Reinforcement Learning Based Adversarial Malware Example Generation against Black-Box Detectors},
  author = {Zhong, Fangtian and Hu, Pengfei and Zhang, Guoming and Li, Hong and Cheng, Xiuzhen},
  date = {2022-10-01},
  journaltitle = {Computers \& Security},
  shortjournal = {Computers \& Security},
  volume = {121},
  pages = {102869},
  issn = {0167-4048},
  doi = {10.1016/j.cose.2022.102869},
  url = {https://www.sciencedirect.com/science/article/pii/S0167404822002632},
  urldate = {2024-09-14},
  abstract = {Recent advances in machine learning offer attractive tools for sophisticated adversaries. An attacker could transform malware into its adversarial version but retain its malicious functionality by employing a dedicated perturbation method. These adversarial malware examples have demonstrated the effectiveness to bypass antivirus engines. However, recent works only leverage a single perturbation method to generate adversarial examples, which cannot defeat advanced detectors. In this paper, we propose a reinforcement learning-based framework called MalInfo, which could generate powerful adversarial malware examples to evade the third-party detectors via an adaptive selection of a perturbation path for each malware in our collected dataset with 1000 diverse malware. To cope with limited computation, MalInfo applies either dynamic programming or temporal difference learning to choose the optimal perturbation path where each path is formed by the combination of Obfusmal, Stealmal, and Hollowmal. We provide a proof-of-concept implementation and extensive evaluation of our framework. Both the detection rate and evasive rate have substantially been improved compared with the state-of-art research MalFox Zhong et~al. (2021). To be specific, The average detection rates for dynamic programming and temporal difference learning are 23.2\% (21.9\% lower than MalFox) and 27.5\% (7.4\% lower than MalFox), respectively, and the average evasive rates are 65.8\% (17.1\% higher than MalFox) and 59.4\% (5.7\% higher than MalFox), respectively.},
  keywords = {3,Adversarial malware examples,Dynamic programming,Malware,Reinforcement learning,Temporal difference learning},
  file = {C:\Users\liuda\Zotero\storage\S3H8ZSE4\S0167404822002632.html}
}

@article{zhuNgramMalGANEvading2022,
  title = {N-Gram {{MalGAN}}: {{Evading}} Machine Learning Detection via Feature n-Gram},
  shorttitle = {N-Gram {{MalGAN}}},
  author = {Zhu, Enmin and Zhang, Jianjie and Yan, Jijie and Chen, Kongyang and Gao, Chongzhi},
  date = {2022-08-01},
  journaltitle = {Digital Communications and Networks},
  shortjournal = {Digital Communications and Networks},
  volume = {8},
  number = {4},
  pages = {485--491},
  issn = {2352-8648},
  doi = {10.1016/j.dcan.2021.11.007},
  url = {https://www.sciencedirect.com/science/article/pii/S2352864821000973},
  urldate = {2024-09-23},
  abstract = {In recent years, many adversarial malware examples with different feature strategies, especially GAN and its variants, have been introduced to handle the security threats, e.g., evading the detection of machine learning detectors. However, these solutions still suffer from problems of complicated deployment or long running time. In this paper, we propose an n-gram MalGAN method to solve these problems. We borrow the idea of n-gram from the Natural Language Processing (NLP) area to expand feature sources for adversarial malware examples in MalGAN. Generally, the n-gram MalGAN obtains the feature vector directly from the hexadecimal bytecodes of the executable file. It can be implemented easily and conveniently with a simple program language (e.g., C++), with no need for any prior knowledge of the executable file or any professional feature extraction tools. These features are functionally independent and thus can be added to the non-functional area of the malicious program to maintain its original executability. In this way, the n-gram could make the adversarial attack easier and more convenient. Experimental results show that the evasion rate of the n-gram MalGAN is at least 88.58\% to attack different machine learning algorithms under an appropriate group rate, growing to even 100\% for the Random Forest algorithm.},
  keywords = {7,Adversarial examples,Machine learning,MalGAN,N-gram},
  file = {C:\Users\liuda\Zotero\storage\IH726JGY\S2352864821000973.html}
}
