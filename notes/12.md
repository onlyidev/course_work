# S3QR

- Survey

  - The authors propose a new GAN extension that would maximize the mutual information between a small subset of the latent variables and observation

  - The proposed model is able to learn disentangled representations in an unsupervised manner

  - The authors seems confident to have found a simple way of ensuring the disentangled representations\* are interpretable and meaningful without prior knowledge of downstream tasks

- Question

  - what is the key modification to the GAN?

    - Add latent codes to the noise and make sure the generator is forced (via the cost function) to include them

  - are the feature labels extractable?

    - Not automatically. The inputs to the generator can be tweaked and meaningful semantic changes can be observed.

* * *

# Meta Info

**Research Question:** How can we make unsupervised learning extract unknown hidden features that are interpretable and meaningful?

**Context:** There exist a number of unsupervised learning methods that can extract disentangled representations. Most of them work on semi-supervised datasets and all strive to extract the representation (InfoGAN does not). Variational autoencoders (VAEs) and GANs have shown to be the most prominent unsupervised generative learning techniques.

* * *

# Notes

InfoGAN can disentangle both discrete and continuous latent factors.

GANs may use noise without restrictions. This means that the latent space may be highly entangled (i.e. not representative of the semantic features)

## The modification

The traditional GAN is modified in such a way:

- the noise input is no longer random, it consists of

  - incompressible noise

  - latent code (some semantic feature vector)

- the GAN is forced to keep the mutual information between the generated sample and the latent code high (the latent code information must not be lost). To achieve this, the generator cost function is modified by subtracting the mutual information (difference of two entropy terms) from the original implementation.

- The previous cost modification requires access to the posterior (the actual distribution of features). We do not have that information, but the authors provide a way to obtain the lower bound for it. Therefore, the cost function is modified to subtract the mutual information not using the true feature distribution, but rather the variational lower bound.

The latent codes are provided in the form of an array of distributions and whether they are discrete or continuous.
