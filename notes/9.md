# S3QR

- Survey

  - The recognize that RL can be used to evade static ML anti-malware detectors

  - The authors want to be able to generate functioning adversarial examples, which is not possible with gradient-based approaches

  - The authors are confident they can produce a framework that is functional on black-box models that do not report a score

  - The authors wish to accurately represent realistic attack scenarios

- Question

  - Does the presented method use a surrogate model?

    - No, the model itself does not use a surrogate.

  - Describe the learning process

    - The agent performs a single mutation (selects an action) given the current state (feature vector) and the Q values

    - The modified malware is given to the detector

    - The detector returns either R (if evaded) or 0 (if not evaded)

    - The agent adjusts (Some steps might be missing. Need to read more about ACER)

    - The cycle continues

  - How is valid execution of the generated samples ensured?

    - Action space is non-breaking

    - In some cases it is not ensured. This happens when the malware is PE-invalid

  - Is and how the original objectives of the malware ensured?

    - Action space is functionality-preserving

* * *

# Meta Info

**Research Question:** How might we generalize static PE anti-malware evasion to be able to trick black-box models?

**Context:**

- Same authors had previously released a whitepaper on the same topic. This paper offers additional information and discusses the known limitations

- Gradient-based approaches have previously been researched

- The features extracted from PE may continue to be relevant for a long time due to the structured format and backwards-compatibility requirements

* * *

# Notes

We can assume the following attack scenarios:

- Direct gradient (attacker has full access to the modelâ€™s weights - easiest and unrealistic scenario)

- White-box (attacker has access to the confidence score of the model - mid scenario)

- Black-box

  - we assume the features (due to PE format usually)

  - we assume no knowledge (hardest and most realistic)

ACER models (actor-critic with experience replay)

[https://github.com/endgameinc/gym-malware](https://github.com/endgameinc/gym-malware)

RL with extremely large action spaces is a subject of ongoing research

The Agent utilizes Boltzman exploration strategy

Only 10 rounds without a reward are allowed

- Long sequences may lead to complications in training (the credit assignment problem)

- The authors wish to avoid degenerate sequences (e.g. an empty section is added n times, where n is all steps)

The authors found that the Agent policy and Random policy produce similar evasion results

After retraining on the newly discovered samples by RL, the original malware detector (GBDT) was able to detect more holdout samples (evasion rate dropped by 33%)

The authors discovered that some of their functionality-preserving modifications actually break the file and it becomes no longer valid. This is due to uncommon practices and obfuscation methods used by malware.
