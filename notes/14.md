# S3QR

- Survey

  - The authors seem confident to have created a truly generic black-box attack model

  - The authors seem wary of GANs

  - The paper focuses heavily on RNN-based ML detectors

  - The paper focuses on API calls

- Question

  - What are dynamic features?

    - API sequence calls (Cuckoo Sandbox was used)

  - How are dynamic detectors evaded?

    - via adversarial attacks (the authors do not consider specific logic that detects a sample being run in a VM as that is not generic enough)

  - How is efficiency ensured?

    - Sliding window technique is used

    - Jacobian heuristic should keep the added API sequence short

  - What is the underlying architecture of the evasion model?

    - It is not a neural network, but an algorithm, that simply uses the Jacobian heuristic and the surrogate model to add API calls to the sequence.

* * *

# Meta Info

**Research Question:** How might we make a black-box attack model that would work against LSTM, RNN and other ML-based malware classifiers, including ones extracting dynamic features, reliably and efficiently?

**Context:**

- GAN architectures are not very reliable as their training process is unstable (this causes very noticeable problems, when datasets are large, e.g. 500k)

- Most papers assume white-box scenarios

- Most papers focus on a specific detector network type

- Most papers rely on the transferability principle

- Most papers do not analyze dynamic features

* * *

# Notes

The paper uses an API-call based malware classifier (treated as “the real classifier”), which is a bit strange, as API calls are static features.

The surrogate model uses the Jacobian heuristic to find the black-box models decision boundaries

The adversarial example in this case is a sequence of added API calls (API calls cannot be removed as that may break functionality)

The authors used Cuckoo Sandbox to extract API calls as features

The authors do not consider malware that is able to detect when ran in a VM

It is not clear what are “dynamic features” are. Although, it seems that API call sequence (extracted from cuckoo sandbox) might be it

The authors developed their own dynamic classifier as there were none reasonably available.

Traditional ML models were able to classify adversarial examples as malware (correctly) more effectively due to a large gap between their and the surrogate model’s decision boundaries

The authors do not address PE-invalid cases

The GADGET framework also proposes to extend the API calls features to “API calls with their arguments” features

The generation of adversarial examples is implemented as a *mimicry attack*.

The attack algorithm used by the authors first selects a random API to add, then it uses the surrogate model information (the Jacobian heuristic) to get a sense of direction which might yield better evasion results. The API calls are added while the sample is classified as malicious.
