# S3QR

- Survey

  - The authors encompass and unify previous research by proposing the RAMEN framework, which expresses white-box and black-box adversarial attacks on static code analysis based malware detectors

  - The authors propose 3 novel attack (obfuscation) strategies

- Question

  - How do the 3 proposed strategies solve the issue of PE-invalid malware (problem mentioned in #8 and #9)?

    - They do not. The authors do not mention anything about PE-invalid malware, although they do reference #8

  - Which approach does RAMEN use (GAN/RL/Genetic/Other) and why?

    - RAMEN defines a framework that allows for any of these models to be adapted. However, the authors themselves seem to be keen on using genetic algorithms.

  - Does RAMEN handle white-box and black-box cases differently?

    - Not really. The black-box case is transformed into the white-box case with the use of a surrogate model.

  - Does RAMEN rely on subject-matter knowledge, or is it a general raw-bytes approach?

    - It can be adapted to both. However, the novel strategies proposed by the authors are all byte-based.

* * *

# Meta Info

**Research Question:** How might we unify, generalize and improve existing adversarial attack models?

**Context:**

- Malware classifiers usually use feature maps to vectors that are then processed by CNNs

- 2 leading architectures are well-researched: MalConv, which lacks robustness due to weak spatial locality and DNNs with enforced spatial locality among features.

- Ensemble models, such as gradient boosted decision tree (GBDT) work quite well, but are also vulnerable to evasion attacks (although computationally more expensive)

* * *

# Notes

The authors share their work at [https://github.com/pralab/secml_malware](https://github.com/pralab/secml_malware)

The PE format allows to extract some useful information about the program without executing it.

Deep learning malware classifiers' overarching architecture is key to their resilience to various adversarial attacks.

In White-box attacks, the embedding step is non-differentiable. Therefore, the optimization step alters the feature step and other methods are used to remap the altered state to the input space

Due to the reconstruction problem, the transformed result in the input space may not exactly represent the optimized feature-space result, the process may need to be repeated a few times before it starts showing improved evasion results.

Black box attacks are useful in:

- black-box scenarios

- non-differentiable detector models (decision trees, random forests)

- recreation step being non-efficient or just computationally expensive

Adversarial attacks are transferable

For black-box attacks the authors suggest the need for a differentiable surrogate detector model

Since black-box models can only be queried, it is also beneficial to define a query budget  - the maximum number of queries that can be sent to the detector model.

The RAMEN framework presents 3 novel PE modification strategies:

- FullDOS - modified the DOS header keeping MZ and PE position values intact.

- Extend - very similar to FullDOS, except additional steps are taken to enlarge the DOS header, which shifts the PE position (it must be updated)

- Shift - enlarges the section sizes and adds adversarial noise before each actual section start. The offsets to the actual section beginnings need to be adjusted.

Strategies proposed by other research:

- [byte] Header fields - modifications of specific fields in the PE and Optional headers

- [byte] Partial DOS - modifies first 58 bytes between MZ and PE position

- [byte] Slack space - fills padded section content (due to file alignment) with adversarial noise

- [byte] Padding - appends padding bytes at the end of the file

- Section injection - injects a useless section and adds it to the section table. This section can be filled with adversarial noise

- [semantic] API injection - add more APIs to the import table

- [semantic] binary rewriting - changes the code instructions to semantically equivalent

The authors have opted to use a genetic algorithm as the optimizer for black-box gradient-free scenarios.

The proposed novel strategies seem to have good performance against byte-based detector networks. This probably happens due to the novel methods changing the locations of usually static PE elements.

More complex models tend to suffer from transfer attacks, caused by the non-linearity of their decision boundary.

GBDT (Gradient Boosted Decision Tree) is not affected by adversarial transfer attacks. This is likely due to byte-based features only making up a small part of characteristics used by the decision tree.

Another way to achieve evasion is packing - that is compressing the program. The authors see this as “invasive” alteration and do not believe that this fits the adversarial attack research space.

None of the strategies (except for some variants of binary rewriting) discussed in this paper would have an effect against dynamic classifiers.

Mitigation of the discussed novel techniques is possible before classification step. This would essentially erase the work done by RAMEN.
