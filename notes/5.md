# S3QR

- Survey - The authors seem to have come up with a way to generate adversarial malware examples that are able to go undetected by black-box malware detection systems that are based on machine learning models.

- Question

  - How is low detection rate ensured?

    - Low detection rate comes from low TPR of the detectors. It is ensured by frequently training MalGAN to fit the black-box detectors and change the probability distribution of the generator. The main reason why MalGAN is able to achieve 0 TPR is that it has enough memory to accurately fit other models (especially those that are similar in architecture)

  - Are there methods other than retraining that can defend against MalGAN?

    - Defensive Distillation or autoencoders could be used. The authors do not talk much about them.

* * *

# Meta Info

**Research Question:** HMW generate adversarial malware examples such that they are able to avoid detection of machine learning algorithms without knowing the details of implementation?

**Context:**

- ML-based malware detectors tend to be vulnerable to adversarial attacks

- Attackers can figure out which features an ML model is using to differentiate between malware and benignware

- Adversarial examples can transfer between different models

* * *

# Notes

The general architecture for all MalGAN-based approaches is:

- Generator: takes malware features and noise to generate adversarial examples

- Black box detector: Classifies given **features** to malicious or benign

- Substitute detector: used for black-box detector model fitting, therefore the ground-truth labels are taken from the detectors output

The dataset [malwr.com](http://malwr.com) used by this paper is no longer active

API features were used (other papers seemed to prefer DLLs)

The paper emulated a black-box detector. They did not use a real system (like other papers in the future would do)

When MalGAN and the black-box model are trained on the same dataset, MalGAN can reduce TPR to near 0

MalGAN also showed very good results when black-box model and itself were trained on different datasets (lower than 3% TPR)

Malware features are binary. Usually GANs are used for images, which have continuous features

Gradient based algorithms are unable to perform well under black-box conditions (TPR plateau at ~90% [lower is better]), while MalGAN reaches 0 TPR under the same conditions

Retraining is a very effective way to defend against adversarial attacks

Possible ways to defend against adversarial attacks:

- Retraining

- Autoencoders

- Defensive distillation algorithm

Retraining takes a lot of time to collect and label enough adversarial samples, however, retraining MalGAN after the detectors have been retrained takes much less time and is still as effective as previously, meaning that malware authors retain the advantage in this case.
