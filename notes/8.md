# S3QR

- Survey

  - The authors propose a Genetic Programming model that can generate adversarial examples. They seem confident that this model converges faster than the alternatives, is less complex due to the selected approach and can be reasonably effective (80% cross-evasion rate)

- Question

  - How much less complex is GP compared to state-of-the-art GAN or RL models?

    - I would not say it is less complex. It simply uses a different approach, but the ideas are more or less the same.

  - Are the evasion rates comparable to others' approaches?

    - It is not yet clear. Different articles use different criteria for evasion. This article uses instances of evasion as the metric (# of samples evaded / # of samples). Intuition is hinting that the evasion rates of this approach are lower (worse).

  - Has AIMED been practically tested?

    - Yes, it has been cross-evasion tested with commercial detectors.

  - How does it modify the malware? Do the modifications always preserve the original functionality?

    - Modifications (especially the random mutations) do not always preserve original functionality. This is why the authors express the importance of validating whether the file is functional. As for how the malware is modified - the method is byte-level perturbations (further explained in next week’s reading)

  - What are the feature vectors?

    - Features might not be the correct term. GP takes the entire executable as input and treats byte-level perturbations (Anderson’s works) as genes.

* * *

# Meta Info

**Research Question:** How might we speed up generation of adversarial malware examples and reduce complexity of the models?

**Context:** The authors know of and are motivated by Goodfellow (gradient -based), Hu (MalGAN), Anderson (RL) and others that have conducted research on adversarial learning. Moreover, the ARMED framework has previously been published by the authors. Genetic programming has been studied for decades and seems to be a suitable tool for understanding how sensitive static malware detection is to adversarial examples, as it looks for the best evasion sequence while reducing the search space.

* * *

# Notes

GP does not require previous training time to generate adversarial malware examples (unlike other ML methods)

LIEF - a tool for parsing and modifying different executable formats, including PE.

AIMED workflow:

- Initialize population (manipulation box)

- Evaluate fitness (check functionality, use commercial detectors to see if they classify the example as malware, check for similarity with original malware , prefer more recent members of the population)

- Selection

  - select 2 fittest members

- Crossover

  - Breed the selected members and cross their genes

- Mutation

  - Add random mutations

- Criteria evaluation

- Loop - the parents and the children pass through to the next generation

AIMED uses[https://cuckoosandbox.org/](https://cuckoosandbox.org/)  as the local sandbox

Duplication is allowed later in the later stages of the process. To avoid duplicated genes overtaking the population, a control mechanism that swaps the genes with others that retain the same fitness values, is implemented.

Cloning (or survival of a member in the population) can improve performance, as the fitness score does not need to be recalculated.

The less similar the generated example is from original malware, the better (the authors expect it to better evade malware detectors)

Given that genetic programming, contrary to reinforcement learning, does not add each perturbation sequentially, the budget of perturbations was not used as a metric.

The authors show that it is extremely important to make sure that generates samples are valid (runnable), as 53% of their generated samples were corrupt.

AIMED adversarial examples show good results in cross-evasion (when detector used for training is different than the test detectors)

Around 76% of the generated samples were unmodifiable (max generations was reached and the samples still could not evade the detector)
