\subsection{Skatinamojo mokymosi tipo modelių \glspl{framework}}\label{sec:literature:rl}

Skatinamojo mokymosi (\acs{rl}) modeliai susideda iš agento ir aplinkos. Aplinka susideda iš informatyvių požymių ištraukimo metodo (\textit{angl. feature extraction}) ir kenkėjiškų programų detektoriaus. Agentas -- tai algoritmas ar neuroninis tinklas, kurio tikslas yra surasti optimalią strategiją (\gls{policy}). Šiuo atveju strategija susideda iš perturbacijų (žr. \ref{sec:literature:perturbations}) \citeplace. Bendras \ac{rl} modelių mokymosi etapas yra tokia seka:
\begin{enumerate}
    \item agentas, naudodamas dabartinę aplinkos būseną ir praeito veiksmo atlygį (\textit{angl. reward}), parenka sekantį veiksmą iš galimų veiksmų aibės
    \item atliekamas veiksmas -- perturbuojama programa arba požymių vektorius (priklauso nuo \glswhom{framework})
    \item gaunami aplinkos kitimo įverčiai -- nauja būsena ir atlygis, skaičiuojamas pagal detektoriaus klasifikacijos rezultatą
    \item seka kartojama tol, kol agentas nelaiko strategijos optimalia arba nustatytą kiekį kartų
\end{enumerate}
\citeplace{}